{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLjPUgZ67ghr"
   },
   "source": [
    "# Sarcasm Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2DT1dstWUDg"
   },
   "source": [
    "## Get and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9D0TkX5nVb46",
    "outputId": "69dd00b5-1431-4c12-d400-a9f9104d3e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from gdown) (3.13.3)\n",
      "Requirement already satisfied: requests[socks] in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests[socks]->gdown) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3gQq7rYWWzM",
    "outputId": "d4140ba5-b31b-4225-b3bd-beef573860e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ytPDo88FEC2ArOjdqErAiarAZBNJzEJz\n",
      "To: /home/astrapi69/fake-news-detector/notebook/SarcasmDetect.json\n",
      "100%|██████████████████████████████████████| 6.06M/6.06M [00:00<00:00, 6.57MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1ytPDo88FEC2ArOjdqErAiarAZBNJzEJz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OZM68bF-Wdw6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('./SarcasmDetect.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmRclPlP7gh4"
   },
   "source": [
    "## Remove all records with no headline text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "myWq442E7gh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   is_sarcastic  28619 non-null  int64 \n",
      " 1   headline      28619 non-null  object\n",
      " 2   article_link  28619 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 670.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df[df['headline'] != '']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Lets start with the key objectives\n",
    "\n",
    "# Key Objectives\n",
    "\n",
    "Train-Test Split should be 70:30, use random_state=42 for same data split throughout. Drop the article_link column as it can be used as a leakage to identify fake news directly (theonion makes only fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "X = df['headline']\n",
    "y = df['is_sarcastic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/astrapi69/anaconda3/envs/NLP/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  20   11    7 9790 3001    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 250), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "max_features = 10000  # Maximum vocab size\n",
    "sequence_length = 250  # Maximum length of each headline\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "\n",
    "text_vectorization.adapt(X_train)\n",
    "\n",
    "sample_headline = [\"This is a sample headline\"]\n",
    "print(text_vectorization(sample_headline))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization,\n",
    "    tf.keras.layers.Embedding(max_features + 1, 128),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5154 - loss: 0.6937 - val_accuracy: 0.5190 - val_loss: 0.6840\n",
      "Epoch 2/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5565 - loss: 0.6830 - val_accuracy: 0.5945 - val_loss: 0.6533\n",
      "Epoch 3/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6417 - loss: 0.6398 - val_accuracy: 0.6587 - val_loss: 0.5779\n",
      "Epoch 4/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7438 - loss: 0.5376 - val_accuracy: 0.5262 - val_loss: 1.0149\n",
      "Epoch 5/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7491 - loss: 0.5233 - val_accuracy: 0.8069 - val_loss: 0.4448\n",
      "Epoch 6/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8112 - loss: 0.4268 - val_accuracy: 0.7982 - val_loss: 0.4324\n",
      "Epoch 7/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8069 - loss: 0.4172 - val_accuracy: 0.6521 - val_loss: 0.6722\n",
      "Epoch 8/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8310 - loss: 0.3882 - val_accuracy: 0.8301 - val_loss: 0.3921\n",
      "Epoch 9/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8507 - loss: 0.3526 - val_accuracy: 0.8188 - val_loss: 0.3969\n",
      "Epoch 10/10\n",
      "\u001b[1m627/627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8481 - loss: 0.3493 - val_accuracy: 0.8379 - val_loss: 0.3781\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Initialize the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,           # Log when training is stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]  # Add the EarlyStopping callback here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 - 0s - 1ms/step - accuracy: 0.8379 - loss: 0.3781\n",
      "Test Loss: 0.37808957695961\n",
      "Test Accuracy: 0.8378756046295166\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Confusion Matrix:\n",
      "[[3855  600]\n",
      " [ 792 3339]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      4455\n",
      "           1       0.85      0.81      0.83      4131\n",
      "\n",
      "    accuracy                           0.84      8586\n",
      "   macro avg       0.84      0.84      0.84      8586\n",
      "weighted avg       0.84      0.84      0.84      8586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predict classes with the model\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.round(predictions).astype(int).flatten()  # Round predictions to 0 or 1 and flatten the array\n",
    "\n",
    "# Actual classes\n",
    "actual_classes = y_test.values\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(actual_classes, predicted_classes)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(actual_classes, predicted_classes)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming `word_index` is a dictionary mapping words to their integer index\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# and `embeddings_index` is a dict mapping words to their embedding vector\u001b[39;00m\n\u001b[1;32m      8\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((max_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, embedding_dim))\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, i \u001b[38;5;129;01min\u001b[39;00m word_index\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m max_features:\n\u001b[1;32m     11\u001b[0m         embedding_vector \u001b[38;5;241m=\u001b[39m embeddings_index\u001b[38;5;241m.\u001b[39mget(word)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
